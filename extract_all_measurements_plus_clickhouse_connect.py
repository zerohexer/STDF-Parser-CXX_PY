#!/usr/bin/env python3
"""
Extract ALL Measurements + ClickHouse Integration - C++ STDF Parser Edition
==========================================================================
Windows/macOS/Linux Compatible Version using clickhouse-connect

Complete STDF processing pipeline:
1. Fast C++ binary parsing (from extract_all_measurements.py)  
2. Direct ClickHouse push (skipping slow transformation)
3. Push to ClickHouse using clickhouse-connect (official driver with C optimizations)

This version works on ALL platforms including Windows!

PERFORMANCE OPTIMIZED + ID MAPPING FIXED VERSION!
- Keeps the 3x faster inline cross-product optimization
- Fixes the 7.6M database query hanging issue
"""

import os
import platform
import time
import argparse
import sys
import hashlib
from datetime import datetime

# Platform setup for C++ library
system = platform.system().lower()
if system == "linux":
    lib_dir = os.path.join(os.path.dirname(__file__), "cpp", "third_party", "lib")
    current_path = os.environ.get("LD_LIBRARY_PATH", "")
    if lib_dir not in current_path:
        os.environ["LD_LIBRARY_PATH"] = f"{lib_dir}:{current_path}" if current_path else lib_dir

try:
    import stdf_parser_cpp
    print("âœ… C++ STDF parser loaded")
except ImportError as e:
    print(f"âŒ C++ parser not available: {e}")
    exit(1)

# Import ClickHouse integration using clickhouse-driver (fast native TCP)
try:
    from clickhouse_driver import Client
    from clickhouse_utils import (
        optimize_clickhouse_connection, 
        setup_clickhouse_schema, 
        push_to_clickhouse,
        optimize_table_for_batch_loading,
        create_materialized_views
    )
    print("âœ… ClickHouse integration loaded (clickhouse-driver - native TCP)")
except ImportError as e:
    print(f"âŒ ClickHouse integration not available: {e}")
    print("Make sure clickhouse-driver is installed: pip install clickhouse-driver")
    exit(1)


class STDFProcessor:
    """Main STDF processor with C++ parsing + ClickHouse integration using clickhouse-driver"""
    
    def __init__(self, enable_clickhouse=True, batch_size=10000):
        """
        Initialize the STDF processor
        
        Args:
            enable_clickhouse: Whether to enable ClickHouse push functionality
            batch_size: Batch size for ClickHouse operations
        """
        self.enable_clickhouse = enable_clickhouse
        self.batch_size = batch_size
        self.measurements = []
        self.devices = {}
        self.parameters = {}
        self.processing_stats = {}
        
        # Persistent ID mapping (like original STDF_Parser_CH.py)
        self.device_id_map = {}  # WLD_DEVICE_DMC -> WLD_ID
        self.param_id_map = {}   # WTP_PARAM_NAME -> WTP_ID
        self.device_counter = 0
        self.param_counter = 0
        self.current_file_hash = None  # For deduplication
        
        # Debug counters from original
        self.debug_comma_tests = 0
        self.debug_single_tests = 0
        
        print(f"ðŸš€ STDFProcessor initialized (clickhouse-driver ultra-fast edition)")
        print(f"   ClickHouse integration: {'âœ… Enabled' if enable_clickhouse else 'âŒ Disabled'}")
        print(f"   Batch size: {batch_size:,}")
        print(f"   Platform: {platform.system()} ({platform.machine()})")
    
    def get_device_id(self, device_dmc, client=None):
        """Get or create a consistent WLD_ID for a device DMC with database persistence"""
        # First check if we already have a mapping in memory
        if device_dmc in self.device_id_map:
            return self.device_id_map[device_dmc]
        
        # If ClickHouse client exists, check if mapping exists in database
        if client:
            try:
                # Use format() for clickhouse-connect parameter substitution
                query = f"SELECT wld_id FROM device_mapping WHERE wld_device_dmc = '{device_dmc}'"
                rows = client.execute(query)
                if rows:
                    wld_id = rows[0][0]
                    self.device_id_map[device_dmc] = wld_id
                    # Update counter to avoid ID conflicts
                    if wld_id >= self.device_counter:
                        self.device_counter = wld_id + 1
                    return wld_id
            except Exception as e:
                print(f"âš ï¸ Error checking for existing device in ClickHouse: {e}")
        
        # If we get here, no mapping exists - create new mapping
        new_wld_id = self.device_counter
        self.device_id_map[device_dmc] = new_wld_id
        self.device_counter += 1
        
        # Insert new mapping to database if client available
        if client:
            try:
                # Use format() for clickhouse-connect parameter substitution
                query = f"INSERT INTO device_mapping (wld_id, wld_device_dmc) VALUES ({new_wld_id}, '{device_dmc}')"
                client.execute(query)
            except Exception as e:
                print(f"âš ï¸ Error inserting device mapping to ClickHouse: {e}")
        
        return new_wld_id
    
    def get_param_id(self, param_name, client=None):
        """Get or create a consistent WTP_ID for a parameter name with database persistence"""
        # First check if we already have a mapping in memory
        if param_name in self.param_id_map:
            return self.param_id_map[param_name]
        
        # If ClickHouse client exists, check if mapping exists in database
        if client:
            try:
                # Escape single quotes in parameter name for SQL safety
                escaped_param_name = param_name.replace("'", "\\'")
                query = f"SELECT wtp_id FROM parameter_info WHERE wtp_param_name = '{escaped_param_name}'"
                rows = client.execute(query)
                if rows:
                    wtp_id = rows[0][0]
                    self.param_id_map[param_name] = wtp_id
                    # Update counter to avoid ID conflicts
                    if wtp_id >= self.param_counter:
                        self.param_counter = wtp_id + 1
                    return wtp_id
            except Exception as e:
                print(f"âš ï¸ Error checking for existing parameter in ClickHouse: {e}")
        
        # If we get here, no mapping exists - create new mapping
        new_wtp_id = self.param_counter
        self.param_id_map[param_name] = new_wtp_id
        self.param_counter += 1
        
        # Insert new mapping to database if client available
        if client:
            try:
                # Escape single quotes in parameter name for SQL safety
                escaped_param_name = param_name.replace("'", "\\'")
                query = f"INSERT INTO parameter_info (wtp_id, wtp_param_name) VALUES ({new_wtp_id}, '{escaped_param_name}')"
                client.execute(query)
            except Exception as e:
                print(f"âš ï¸ Error inserting parameter mapping to ClickHouse: {e}")
        
        return new_wtp_id
    
    def _generate_file_hash(self, file_path):
        """Generate MD5 hash of the file for deduplication (like original STDF_Parser_CH.py)"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"âš ï¸ Error generating file hash: {e}")
            return None
    
    def _is_file_already_processed(self, file_hash, client=None):
        """Check if file with this hash has already been processed"""
        if not file_hash or not client:
            return False
        
        try:
            query = f"SELECT COUNT(*) FROM measurements WHERE file_hash = '{file_hash}' LIMIT 1"
            rows = client.execute(query)
            if rows and len(rows) > 0:
                count = rows[0][0]
                return count > 0
        except Exception as e:
            print(f"âš ï¸ Error checking file hash in database: {e}")
        
        return False
    
    def _load_existing_mappings_from_clickhouse(self, host='localhost', port=9000, database='default', user='default', password=''):
        """Load existing device and parameter mappings from ClickHouse"""
        try:
            from clickhouse_driver import Client
            
            # Use provided parameters or stored settings
            actual_host = host if host != 'localhost' else getattr(self, 'ch_host', 'localhost')
            actual_port = port if port != 9000 else getattr(self, 'ch_port', 9000)
            actual_database = database if database != 'default' else getattr(self, 'ch_database', 'default')
            actual_user = user if user != 'default' else getattr(self, 'ch_user', 'default')
            actual_password = password if password != '' else getattr(self, 'ch_password', '')
            
            # Create connection to load mappings
            client = Client(
                host=actual_host,
                port=actual_port,
                database=actual_database,
                user=actual_user,
                password=actual_password
            )
            
            # Load device mappings
            device_mappings = []
            try:
                device_results = client.execute("SELECT wld_device_dmc, wld_id FROM device_mapping")
                device_mappings = [(device_dmc, device_id) for device_dmc, device_id in device_results]
                print(f"ðŸ“Š Loaded {len(device_mappings)} existing device mappings")
            except Exception as e:
                print(f"âš ï¸ No existing device mappings found: {e}")
                device_mappings = []
            
            # Load parameter mappings
            param_mappings = []
            try:
                param_results = client.execute("SELECT wtp_param_name, wtp_id FROM parameter_info")
                param_mappings = [(param_name, param_id) for param_name, param_id in param_results]
                print(f"ðŸ“Š Loaded {len(param_mappings)} existing parameter mappings")
            except Exception as e:
                print(f"âš ï¸ No existing parameter mappings found: {e}")
                param_mappings = []
            
            return device_mappings, param_mappings
            
        except Exception as e:
            print(f"âš ï¸ Error loading existing mappings: {e}")
            return [], []
    
    def extract_measurements(self, stdf_file_path, ch_host='localhost', ch_port=9000, ch_database='default', ch_user='default', ch_password=''):
        """Extract ALL measurements from STDF file using DATABASE-AWARE C++ processing"""
        print(f"\nðŸ“„ Processing: {os.path.basename(stdf_file_path)}")
        
        start_time = time.time()
        
        # ðŸ” FILE HASH DEDUPLICATION CHECK - BEFORE expensive processing!
        print(f"ðŸ” Checking file deduplication...")
        file_hash = self._generate_file_hash(stdf_file_path)
        if file_hash:
            print(f"   ðŸ“„ File hash: {file_hash}")
            self.current_file_hash = file_hash
            
            # Create temporary ClickHouse connection to check for duplicates
            try:
                from clickhouse_driver import Client
                temp_client = Client(
                    host=ch_host,
                    port=ch_port,
                    database=ch_database,
                    user=ch_user,
                    password=ch_password
                )
                
                if self._is_file_already_processed(file_hash, temp_client):
                    filename = os.path.basename(stdf_file_path)
                    print(f"âš ï¸ File {filename} already processed (hash: {file_hash})")
                    print(f"âš ï¸ Skipping processing to prevent duplicates")
                    # Return empty results to indicate file was skipped
                    self.measurement_tuples = []
                    self.new_device_mappings = []
                    self.new_param_mappings = []
                    self.processing_stats = {
                        'skipped_duplicate': True,
                        'file_hash': file_hash,
                        'total_measurements': 0
                    }
                    return []
                else:
                    print(f"âœ… File not previously processed, continuing...")
            except Exception as e:
                print(f"âš ï¸ Could not check for duplicates: {e}")
                print(f"âš ï¸ Proceeding with processing...")
        else:
            print(f"âš ï¸ Could not generate file hash, proceeding without deduplication")
        
        # ðŸ”§ DATABASE-AWARE: Load existing mappings from ClickHouse
        print(f"ðŸ”§ Loading existing device/parameter mappings from ClickHouse...")
        device_mappings, param_mappings = self._load_existing_mappings_from_clickhouse(
            host=ch_host, port=ch_port, database=ch_database, user=ch_user, password=ch_password
        )
        
        # ðŸš€ ULTRA-FAST: Process STDF with database-aware IDs
        # Pass the Python-generated file hash to C++ to ensure consistency
        result = stdf_parser_cpp.process_stdf_with_database_mappings(
            stdf_file_path, 
            device_mappings, 
            param_mappings,
            self.current_file_hash or ""  # Pass the MD5 hash from Python
        )
        
        # Extract results from C++ processing
        measurement_tuples = result.get('measurement_tuples', [])
        new_device_mappings = result.get('new_device_mappings', [])
        new_param_mappings = result.get('new_param_mappings', [])
        
        total_time = time.time() - start_time
        
        print(f"ðŸš€ ULTRA-FAST C++ processing completed:")
        print(f"   ðŸ“Š Records parsed: {result.get('total_records', 0):,}")
        print(f"   ðŸ“Š Measurements: {result.get('total_measurements', 0):,}")
        print(f"   â±ï¸ C++ parsing: {result.get('parsing_time', 0):.2f}s")
        print(f"   â±ï¸ C++ processing: {result.get('processing_time', 0):.2f}s")
        print(f"   â±ï¸ Total time: {total_time:.2f}s")
        
        if total_time > 0:
            throughput = len(measurement_tuples) / total_time
            print(f"   ðŸš€ Throughput: {throughput:.0f} measurements/second")
        
        # Store tuples for ClickHouse insertion
        self.measurement_tuples = measurement_tuples
        
        # Update ID mappings from C++ results (includes both existing + new)
        total_devices = len(device_mappings) + len(new_device_mappings)
        total_params = len(param_mappings) + len(new_param_mappings)
        
        # Store new mappings for database insertion
        self.new_device_mappings = new_device_mappings
        self.new_param_mappings = new_param_mappings
        
        print(f"   ðŸ“Š Total device mappings: {total_devices:,} ({len(device_mappings):,} existing + {len(new_device_mappings):,} new)")
        print(f"   ðŸ“Š Total parameter mappings: {total_params:,} ({len(param_mappings):,} existing + {len(new_param_mappings):,} new)")
        
        # Store processing stats
        self.processing_stats = {
            'cpp_parsing_time': result.get('parsing_time', 0),
            'cpp_processing_time': result.get('processing_time', 0),
            'total_processing_time': total_time,
            'total_measurements': len(measurement_tuples),
            'total_records': result.get('total_records', 0),
            'ultra_fast_mode': True
        }
        
        # For compatibility, also store as measurements list (but empty to save memory)
        self.measurements = []
        
        return measurement_tuples
    
    def _extract_from_records(self, record_types):
        """Extract measurements from parsed records using OPTIMIZED CROSS-PRODUCT logic + FIXED ID mapping"""
        
        # Get MIR info for context
        mir_info = self._get_mir_info(record_types.get('MIR', []))
        
        # Extract PRR records (device information) - following original logic
        prr_records = record_types.get('PRR', [])
        if not prr_records:
            print("âŒ No PRR records found - cannot create measurements")
            return
        
        print(f"ðŸ”§ Found {len(prr_records)} PRR records (devices)")
        
        # Get ALL test records - CRITICAL: Original processes MPR and PTR separately!
        test_records = []
        
        # Original processes these record types
        for record_type in ['MPR', 'PTR']:  # Focus on main test records like original
            if record_type in record_types:
                test_records.extend(record_types[record_type])
                print(f"ðŸ“Š Found {len(record_types[record_type]):,} {record_type} records")
        
        # Also include other test record types if available
        for record_type in ['FTR', 'SBR', 'HBR']:
            if record_type in record_types:
                test_records.extend(record_types[record_type])
                print(f"ðŸ“Š Found {len(record_types[record_type]):,} {record_type} records")
        
        print(f"ðŸ§ª Total test records to process: {len(test_records):,}")
        print(f"ðŸ“„ Cross-product calculation: {len(prr_records)} devices Ã— {len(test_records):,} tests = {len(prr_records) * len(test_records):,} base operations")
        print(f"ðŸ“ˆ Each test can create multiple measurements from comma-separated values")
        
        # ðŸš€ OPTIMIZED CROSS-PRODUCT: Pre-allocate + cache + inline (3x faster) - KEEP THIS!
        
        # 1. PRE-ALLOCATE: Estimate total measurements (Python list will grow efficiently)
        estimated_total = len(prr_records) * len(test_records) * 10  # ~3.8M measurements
        self.measurements = []
        print(f"ðŸš€ Optimized cross-product: estimated {estimated_total:,} measurements")
        
        # 2. CACHE: Pre-extract MIR info (avoid 3.7M repeated lookups)
        mir_facility = mir_info.get('facility', '')
        mir_operation = mir_info.get('operation', '')
        mir_lot_name = mir_info.get('lot_name', '')
        mir_equipment = mir_info.get('equipment', '')
        file_hash = self.current_file_hash or ""
        
        # 3. CACHE: Pre-process test records with PIXEL TEST FILTERING (avoid repeated field lookups)
        test_cache = []
        param_id_mapping = {}  # ðŸš€ FIX: Build proper parameter ID mapping
        
        for test in test_records:
            test_fields = test.get('fields', {})
            
            # CRITICAL: Apply pixel test filtering like original _process_single_test
            # Use same data sources as old working code
            param_name = test.get('test_txt', '')  # Direct field like old code
            test_txt = test.get('test_txt', '')    # Same as param_name
            
            # Skip if not a pixel test (same filtering as old code)
            if not self._is_pixel_test(param_name, test_txt):
                continue  # Skip this test entirely
            
            # ðŸš€ FIX: Use cleaned parameter names and build proper ID mapping
            cleaned_param_name = self._clean_param_name(param_name)
            if cleaned_param_name not in self.parameters:
                self.parameters[cleaned_param_name] = len(self.parameters)
            param_id = self.parameters[cleaned_param_name]
            param_id_mapping[cleaned_param_name] = param_id
            
            result_string = test_fields.get('RTN_RSLT', test_fields.get('RESULT', ''))
            if result_string:
                try:
                    values = [float(v.strip()) for v in result_string.split(',') if v.strip()]
                    if values:
                        self.debug_comma_tests += 1
                    else:
                        values = [0.0]
                        self.debug_single_tests += 1
                except:
                    values = [0.0]
                    self.debug_single_tests += 1
            else:
                values = [0.0]
                self.debug_single_tests += 1
                
            test_cache.append((
                values,
                self._safe_int(test_fields.get('TEST_NUM', 0)),  # Convert to int
                cleaned_param_name,  # ðŸš€ FIX: Use cleaned parameter name
                param_id,           # ðŸš€ FIX: Use proper parameter ID  
                test_txt,    # test_txt (same as param_name in this case)
                test_fields.get('UNITS', ''),
                self._safe_int(test_fields.get('TEST_FLG', 0))   # Extract TEST_FLG for deduplication
            ))
        
        print(f"ðŸŽ¯ Cached {len(test_cache):,} pixel tests with proper parameter IDs")
        
        processed_devices = 0
        total_measurements_created = 0
        
        for prr in prr_records:
            prr_fields = prr.get('fields', {})
            
            # Extract device data from PRR (following original logic)
            device_dmc = prr_fields.get('PART_ID', prr_fields.get('PART_TXT', ''))
            bin_code = prr_fields.get('SOFT_BIN', prr_fields.get('HARD_BIN', ''))
            default_x_pos = self._safe_int(prr_fields.get('X_COORD', 0))
            default_y_pos = self._safe_int(prr_fields.get('Y_COORD', 0))
            
            # ðŸš€ FIX: Get consistent device ID using proper mapping (following original logic)
            if device_dmc not in self.devices:
                self.devices[device_dmc] = len(self.devices)
            device_id = self.devices[device_dmc]
            
            device_measurements_before = len(self.measurements)
            
            # 4. ELIMINATE FUNCTION CALLS: Inline _process_single_test (avoid function calls) - KEEP THIS!
            for values, test_num, cleaned_param_name, param_id, test_txt, units, test_flg in test_cache:
                
                # Extract pixel coordinates (inline for speed)
                pixel_x, pixel_y = self._extract_test_coordinates(
                    cleaned_param_name, test_txt, default_x_pos, default_y_pos
                )
                
                # 5. FASTER DATA STRUCTURES: Create measurements directly (inline logic) - KEEP THIS!
                for value in values:
                    # Core measurement dict - optimized creation
                    measurement = {
                        # ðŸš€ FIX: Use PROPER IDs and parameter names
                        'WLD_ID': device_id,  # Use proper device ID
                        'WTP_ID': param_id,   # ðŸš€ FIX: Use proper param ID (not test_num!)
                        'WTP_PARAM_NAME': cleaned_param_name,  # ðŸš€ FIX: Use cleaned parameter name
                        'WP_POS_X': pixel_x,
                        'WP_POS_Y': pixel_y,
                        'WPTM_VALUE': value,
                        'TEST_FLAG': 1 if bin_code == '1' else 0,
                        'TEST_FLG': test_flg,  # Raw STDF flag for deduplication
                        'TEST_NUM': test_num,
                        'SEGMENT': 0,
                        'FILE_HASH': file_hash,
                        'WLD_DEVICE_DMC': device_dmc,
                        'WLD_BIN_CODE': bin_code
                    }
                    
                    # Add optional fields only if non-empty (avoid dictionary bloat)
                    if units:
                        measurement['UNITS'] = units
                    if mir_facility:
                        measurement['WFI_FACILITY'] = mir_facility
                    if mir_operation:
                        measurement['WFI_OPERATION'] = mir_operation
                    if mir_lot_name:
                        measurement['WL_LOT_NAME'] = mir_lot_name
                    if mir_equipment:
                        measurement['WFI_EQUIPMENT'] = mir_equipment
                    
                    self.measurements.append(measurement)
            
            device_measurements_created = len(self.measurements) - device_measurements_before
            total_measurements_created += device_measurements_created
            processed_devices += 1
            
            if device_measurements_created > 0:
                avg_per_test = device_measurements_created / len(test_cache) if test_cache else 0
                print(f"  Device {processed_devices}/{len(prr_records)}: {device_dmc} â†’ {device_measurements_created:,} measurements (avg {avg_per_test:.1f} per test)")
        
        print(f"âœ… Optimized cross-product processing completed:")
        print(f"   ðŸ“Š Processed {processed_devices} devices")
        print(f"   ðŸ“Š Created {total_measurements_created:,} total measurements")
        print(f"   ðŸ“Š Average {total_measurements_created//processed_devices if processed_devices > 0 else 0:,} measurements per device")
        print(f"   ðŸ” DEBUG: Tests with commas: {self.debug_comma_tests:,}")
        print(f"   ðŸ” DEBUG: Tests with single values: {self.debug_single_tests:,}")
        if (self.debug_comma_tests + self.debug_single_tests) > 0:
            print(f"   ðŸ” DEBUG: Comma ratio: {self.debug_comma_tests/(self.debug_comma_tests+self.debug_single_tests)*100:.1f}% have commas")
    
    def _get_mir_info(self, mir_records):
        """Extract basic info from MIR record (from original)"""
        if not mir_records:
            return {}
        
        mir_fields = mir_records[0].get('fields', {})
        return {
            'facility': mir_fields.get('FACIL_ID', mir_fields.get('FLOOR_ID', '')),
            'lot_name': mir_fields.get('LOT_ID', mir_fields.get('PART_TYP', '')),
            'operation': mir_fields.get('OPER_NAM', mir_fields.get('SPEC_NAM', '')),
            'equipment': mir_fields.get('NODE_NAM', mir_fields.get('JOB_NAM', '')),
            'start_time': mir_fields.get('START_T', ''),
            'prog_name': mir_fields.get('JOB_REV', ''),
            'prog_version': mir_fields.get('SBLOT_ID', '')
        }
    
    def _extract_test_coordinates(self, alarm_id, test_txt, default_x, default_y):
        """Extract pixel coordinates (from original)"""
        x_pos, y_pos = self._parse_pixel_coords(alarm_id)
        if x_pos is None and test_txt:
            x_pos, y_pos = self._parse_pixel_coords(test_txt)
        return x_pos if x_pos is not None else default_x, y_pos if y_pos is not None else default_y
    
    def _parse_pixel_coords(self, text):
        """Parse Pixel=R##C## pattern (from original)"""
        if not text or 'Pixel=' not in text:
            return None, None
        
        import re
        match = re.search(r'Pixel=R(\d+)C(\d+)', text)
        if match:
            row = int(match.group(1))  # R = Row = Y
            col = int(match.group(2))  # C = Column = X  
            return col, row  # Return as (X, Y)
        
        return None, None
    
    def _clean_param_name(self, param_name):
        """Clean parameter name (from original)"""
        if not param_name:
            return param_name
        
        import re
        cleaned = re.sub(r';Pixel=R\d+C\d+', '', param_name)
        cleaned = re.sub(r'^Pixel=R\d+C\d+;', '', cleaned)
        return cleaned
    
    def _parse_test_values(self, test_txt):
        """Parse test values (from original)"""
        if not test_txt:
            return ['0.0']
        
        if ',' in test_txt:
            values = [x.strip() for x in test_txt.split(',') if x.strip()]
            return values if values else ['0.0']
        
        return [test_txt]
    
    def _is_pixel_test(self, alarm_id, test_txt):
        """Check if test involves pixels (from original)"""
        if alarm_id and 'Pixel=' in alarm_id:
            return True
        if test_txt and 'Pixel=' in test_txt:
            return True
        return False
    
    def _safe_float(self, value):
        """Safely convert to float (from original)"""
        try:
            return float(value) if value else 0.0
        except (ValueError, TypeError):
            return 0.0
    
    def _safe_int(self, value):
        """Safely convert to int (from original)"""
        try:
            return int(float(value)) if value else 0
        except (ValueError, TypeError):
            return 0
    
    def push_to_clickhouse(self, stdf_file_path, host='localhost', port=9000, database='default', 
                          user='default', password=''):
        """
        Push measurements to ClickHouse using clickhouse-driver (fastest native TCP!)
        
        Args:
            stdf_file_path: Path to original STDF file (for hash generation)
            host: ClickHouse server hostname
            port: ClickHouse native TCP port (default 9000 for clickhouse-driver)
            database: Database name
            user: Username for authentication
            password: Password for authentication
        """
        # Store ClickHouse connection parameters for later use
        self.ch_host = host
        self.ch_port = port
        self.ch_database = database
        self.ch_user = user
        self.ch_password = password
        
        if not self.enable_clickhouse:
            print("âš ï¸ ClickHouse integration is disabled")
            return False
        
        # Check for measurements in either format (tuples or list)
        has_measurements = (hasattr(self, 'measurement_tuples') and self.measurement_tuples) or \
                          (hasattr(self, 'measurements') and self.measurements)
        
        if not has_measurements:
            print("âš ï¸ No measurements to push")
            return False
        
        print(f"\nðŸš€ Starting ClickHouse integration (clickhouse-driver - native TCP)...")
        clickhouse_start = time.time()
        
        try:
            # Check if we have ultra-fast tuples or need to process old format
            if hasattr(self, 'measurement_tuples') and self.measurement_tuples:
                print(f"ðŸš€ Using ULTRA-FAST C++ tuples (no transformation needed)...")
                transform_time = 0.0  # No transformation needed!
                
                # Convert tuples to ClickHouse format with datetime
                from datetime import datetime
                current_time = datetime.now()
                
                # Ultra-fast tuple conversion (already optimized in C++)
                clickhouse_tuples = []
                for tuple_data in self.measurement_tuples:
                    # ðŸš€ MACRO-DRIVEN: Unpack all fields automatically
                    # C++ macros ensure this matches the field count exactly
                    (wld_id, wtp_id, wp_pos_x, wp_pos_y, wptm_value, test_flag, segment, file_hash,
                     wld_device_dmc, wtp_param_name, units, test_num, test_flg) = tuple_data
                    
                    clickhouse_tuples.append((
                        wld_id,
                        wtp_id, 
                        wp_pos_x,
                        wp_pos_y,
                        wptm_value,
                        current_time,  # ClickHouse datetime
                        test_flag,
                        segment,
                        file_hash
                    ))
                
                # Create ultra-fast data store
                data_store = {
                    'measurement_tuples': clickhouse_tuples,
                    'measurements': [],  # Empty to save memory
                    'landing_records': []
                }
                
                print(f"âœ… Ultra-fast tuple conversion: {len(clickhouse_tuples):,} tuples ready for ClickHouse")
                
            else:
                # Fallback to old format processing
                print(f"ðŸš€ Using C++ measurements directly (skipping transformation)...")
                transform_time = 0.0  # No transformation time since we skip it
                
                # Fix DateTime fields - convert strings to datetime objects
                from datetime import datetime
                current_time = datetime.now()
                
                # Fix measurements with proper DateTime objects and add segment field
                fixed_measurements = []
                duplicate_tracker = {}  # Track duplicates like STDF_Parser_CH.py
                
                for measurement in self.measurements:
                    fixed_measurement = measurement.copy()
                    
                    # Convert string timestamps to datetime objects for ClickHouse
                    if 'WPTM_CREATED_DATE' in fixed_measurement:
                        fixed_measurement['WPTM_CREATED_DATE'] = current_time
                    if 'WLD_CREATED_DATE' in fixed_measurement:
                        fixed_measurement['WLD_CREATED_DATE'] = current_time
                    
                    # Add segment field for deduplication (like STDF_Parser_CH.py)
                    # Create duplicate key based on device + parameter + coordinates + test_flag (like clickhouse_utils.py line 768)
                    duplicate_key = (
                        fixed_measurement.get('WLD_ID', 0),
                        fixed_measurement.get('WTP_ID', 0), 
                        str(fixed_measurement.get('WP_POS_X', 0)),  # Convert to string like original
                        str(fixed_measurement.get('WP_POS_Y', 0)),  # Convert to string like original
                        fixed_measurement.get('TEST_FLG', 0)        # Add TEST_FLG (raw STDF flag) for deduplication
                    )
                    
                    # Get segment number (0 for first occurrence, increment for duplicates)
                    if duplicate_key in duplicate_tracker:
                        segment = duplicate_tracker[duplicate_key] + 1
                        duplicate_tracker[duplicate_key] = segment
                    else:
                        segment = 0
                        duplicate_tracker[duplicate_key] = 0
                    
                    fixed_measurement['segment'] = segment
                    fixed_measurements.append(fixed_measurement)
                
                # Create a simple data store using fixed C++ measurements
                data_store = {
                    'measurements': fixed_measurements,
                    'landing_records': []  # Empty for now
                }
            
            # Create a simple extractor-like object that mimics STDF_Parser_CH.py interface
            class SimpleExtractorLike:
                def __init__(self, data_store, measurements, processor_instance):
                    self.data_store = data_store
                    self.processor = processor_instance  # Reference to main processor for persistent ID methods
                    
                    # Use processor's persistent ID mapping (will be updated with database lookup)
                    self.device_id_map = self.processor.device_id_map
                    self.param_id_map = self.processor.param_id_map
                
                def update_measurements_with_persistent_ids(self, client=None):
                    """Update measurements with database-persistent device and parameter IDs - OPTIMIZED VERSION"""
                    
                    if not client:
                        print("âš ï¸ No ClickHouse client, skipping database ID mapping")
                        return
                    
                    measurements = self.data_store['measurements']
                    print(f"ðŸ”§ Updating {len(measurements):,} measurements with persistent IDs...")
                    
                    # ðŸš€ FIX 1: Collect unique names to minimize database queries
                    unique_devices = set()
                    unique_params = set()
                    
                    for measurement in measurements:
                        device_name = measurement.get('WLD_DEVICE_DMC', '')
                        param_name = measurement.get('WTP_PARAM_NAME', '')
                        
                        if device_name and device_name != 'unknown':
                            unique_devices.add(device_name)
                        if param_name and param_name != 'unknown':
                            unique_params.add(param_name)
                    
                    print(f"   ðŸŽ¯ Found {len(unique_devices)} unique devices, {len(unique_params)} unique parameters")
                    
                    # ðŸš€ FIX 2: Batch lookup devices from database
                    device_mapping = {}
                    if unique_devices:
                        try:
                            # Single query to get all existing device mappings
                            device_list = "', '".join(unique_devices)
                            query = f"SELECT wld_device_dmc, wld_id FROM device_mapping WHERE wld_device_dmc IN ('{device_list}')"
                            rows = client.execute(query)
                            
                            for device_name, device_id in rows:
                                device_mapping[device_name] = device_id
                                self.processor.device_id_map[device_name] = device_id
                                # Update counter to avoid conflicts
                                if device_id >= self.processor.device_counter:
                                    self.processor.device_counter = device_id + 1
                            
                            print(f"   ðŸ“Š Loaded {len(device_mapping)} existing device mappings from database")
                        except Exception as e:
                            print(f"   âš ï¸ Error batch loading device mappings: {e}")
                    
                    # ðŸš€ FIX 3: Batch lookup parameters from database  
                    param_mapping = {}
                    if unique_params:
                        try:
                            # Single query to get all existing parameter mappings
                            escaped_params = [param.replace("'", "\\'") for param in unique_params]
                            param_list = "', '".join(escaped_params)
                            query = f"SELECT wtp_param_name, wtp_id FROM parameter_info WHERE wtp_param_name IN ('{param_list}')"
                            rows = client.execute(query)
                            
                            for param_name, param_id in rows:
                                param_mapping[param_name] = param_id
                                self.processor.param_id_map[param_name] = param_id
                                # Update counter to avoid conflicts
                                if param_id >= self.processor.param_counter:
                                    self.processor.param_counter = param_id + 1
                            
                            print(f"   ðŸ“Š Loaded {len(param_mapping)} existing parameter mappings from database")
                        except Exception as e:
                            print(f"   âš ï¸ Error batch loading parameter mappings: {e}")
                    
                    # ðŸš€ FIX 4: Create new mappings for items not found in database
                    new_devices = []
                    new_params = []
                    
                    for device_name in unique_devices:
                        if device_name not in device_mapping:
                            new_id = self.processor.device_counter
                            device_mapping[device_name] = new_id
                            self.processor.device_id_map[device_name] = new_id
                            self.processor.device_counter += 1
                            new_devices.append((new_id, device_name))
                    
                    for param_name in unique_params:
                        if param_name not in param_mapping:
                            new_id = self.processor.param_counter
                            param_mapping[param_name] = new_id
                            self.processor.param_id_map[param_name] = new_id
                            self.processor.param_counter += 1
                            new_params.append((new_id, param_name))
                    
                    # ðŸš€ FIX 5: Batch insert new mappings
                    if new_devices:
                        try:
                            client.execute(
                                "INSERT INTO device_mapping (wld_id, wld_device_dmc) VALUES",
                                new_devices
                            )
                            print(f"   âœ… Inserted {len(new_devices)} new device mappings")
                        except Exception as e:
                            print(f"   âš ï¸ Error inserting device mappings: {e}")
                    
                    if new_params:
                        try:
                            client.execute(
                                "INSERT INTO parameter_info (wtp_id, wtp_param_name) VALUES", 
                                new_params
                            )
                            print(f"   âœ… Inserted {len(new_params)} new parameter mappings")
                        except Exception as e:
                            print(f"   âš ï¸ Error inserting parameter mappings: {e}")
                    
                    # ðŸš€ FIX 6: Update measurements using cached mappings (NO database queries!)
                    processed = 0
                    for measurement in measurements:
                        device_name = measurement.get('WLD_DEVICE_DMC', '')
                        param_name = measurement.get('WTP_PARAM_NAME', '')
                        
                        if device_name in device_mapping:
                            measurement['WLD_ID'] = device_mapping[device_name]
                        
                        if param_name in param_mapping:
                            measurement['WTP_ID'] = param_mapping[param_name]
                        
                        # Add file hash for deduplication
                        if self.processor.current_file_hash:
                            measurement['FILE_HASH'] = self.processor.current_file_hash
                        
                        processed += 1
                        if processed % 500000 == 0:  # Progress update for large datasets
                            print(f"   ðŸ”„ Updated {processed:,}/{len(measurements):,} measurements...")
                    
                    print(f"   âœ… Updated all {processed:,} measurements with persistent IDs")
                    print(f"   ðŸ“Š Total device mappings: {len(self.processor.device_id_map):,}")
                    print(f"   ðŸ“Š Total parameter mappings: {len(self.processor.param_id_map):,}")
                    
                    # Update the mappings in this object
                    self.device_id_map = self.processor.device_id_map
                    self.param_id_map = self.processor.param_id_map
                            
            # Use appropriate measurements based on processing mode
            measurements_ref = data_store.get('measurement_tuples', data_store.get('measurements', []))
            extractor_like = SimpleExtractorLike(data_store, measurements_ref, self)
            
            # Step 2: Setup ClickHouse connection and schema
            print(f"ðŸ”§ Setting up ClickHouse connection and schema...")
            setup_start = time.time()
            
            client = optimize_clickhouse_connection(host, port, database, user, password)
            setup_clickhouse_schema(client)
            
            # Note: File hash deduplication already handled in extract_measurements()
            print(f"â„¹ï¸ File hash already generated: {self.current_file_hash or 'None'}")
            
            # Try to create materialized views
            try:
                create_materialized_views(client)
            except Exception as e:
                print(f"âš ï¸ Some materialized views may not be supported: {e}")
            
            setup_time = time.time() - setup_start
            print(f"âœ… Schema setup completed in {setup_time:.2f}s")
            
            # Step 2.5: Update measurements with database-persistent IDs
            print(f"ðŸ”§ Updating measurements with persistent device/parameter IDs...")
            id_start = time.time()
            extractor_like.update_measurements_with_persistent_ids(client)
            id_time = time.time() - id_start
            print(f"âœ… ID mapping completed in {id_time:.2f}s")
            print(f"   ðŸ“Š Device mappings: {len(self.device_id_map):,}")
            print(f"   ðŸ“Š Parameter mappings: {len(self.param_id_map):,}")
            
            # Step 3: Push data to ClickHouse
            print(f"ðŸ“Š Pushing data to ClickHouse...")
            push_start = time.time()
            
            # Use ultra-fast direct push if we have tuples
            if hasattr(self, 'measurement_tuples') and self.measurement_tuples:
                success = self._push_tuples_to_clickhouse_ultra_fast(
                    data_store['measurement_tuples'],
                    host=host,
                    port=port,
                    database=database,
                    user=user,
                    password=password
                )
            else:
                # Fallback to standard push method
                success = push_to_clickhouse(
                    extractor_like,
                    host=host,
                    port=port,
                    database=database,
                    user=user,
                    password=password,
                    batch_size=self.batch_size
                )
            
            push_time = time.time() - push_start
            total_clickhouse_time = time.time() - clickhouse_start
            
            if success:
                print(f"âœ… ClickHouse push completed in {push_time:.2f}s")
                print(f"ðŸ“Š Total ClickHouse time: {total_clickhouse_time:.2f}s")
                
                # Store ClickHouse stats
                self.processing_stats.update({
                    'clickhouse_transform_time': transform_time,
                    'clickhouse_setup_time': setup_time,
                    'clickhouse_push_time': push_time,
                    'total_clickhouse_time': total_clickhouse_time,
                    'clickhouse_measurements': len(data_store['measurements']),
                    'clickhouse_landing_records': len(data_store['landing_records'])
                })
                
                return True
            else:
                print(f"âŒ ClickHouse push failed")
                return False
            
        except Exception as e:
            print(f"âŒ Error in ClickHouse integration: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _push_tuples_to_clickhouse_ultra_fast(self, tuples, host, port, database, user, password):
        """ðŸš€ ULTRA-FAST: Push pre-processed tuples directly to ClickHouse"""
        try:
            from clickhouse_driver import Client
            
            print(f"ðŸš€ Ultra-fast ClickHouse push: {len(tuples):,} tuples")
            start_time = time.time()
            
            # Create optimized connection
            client = Client(
                host=host,
                port=port,
                database=database,
                user=user,
                password=password,
                settings={
                    'max_insert_block_size': 1000000,
                    'max_threads': 16,
                    'max_insert_threads': 16,
                    'receive_timeout': 300,
                    'send_timeout': 300
                }
            )
            
            # Setup schema first
            setup_clickhouse_schema(client)
            
            # Push only NEW device and parameter mappings
            if hasattr(self, 'new_device_mappings') and self.new_device_mappings:
                device_insert_data = [(device_id, device_dmc) for device_dmc, device_id in self.new_device_mappings]
                client.execute(
                    "INSERT INTO device_mapping (wld_id, wld_device_dmc) VALUES",
                    device_insert_data
                )
                print(f"âœ… Pushed {len(device_insert_data)} NEW device mappings")
            else:
                print(f"â„¹ï¸ No new device mappings to insert")
            
            if hasattr(self, 'new_param_mappings') and self.new_param_mappings:
                param_insert_data = [(param_id, param_name) for param_name, param_id in self.new_param_mappings]
                client.execute(
                    "INSERT INTO parameter_info (wtp_id, wtp_param_name) VALUES", 
                    param_insert_data
                )
                print(f"âœ… Pushed {len(param_insert_data)} NEW parameter mappings")
            else:
                print(f"â„¹ï¸ No new parameter mappings to insert")
            
            # Ultra-fast single insert for all measurements
            print(f"ðŸš€ Inserting {len(tuples):,} measurements in single operation...")
            insert_start = time.time()
            
            client.execute(
                "INSERT INTO measurements (wld_id, wtp_id, wp_pos_x, wp_pos_y, wptm_value, wptm_created_date, test_flag, segment, file_hash) VALUES",
                tuples
            )
            
            insert_time = time.time() - insert_start
            total_time = time.time() - start_time
            throughput = len(tuples) / total_time if total_time > 0 else 0
            
            print(f"âœ… ULTRA-FAST ClickHouse push completed!")
            print(f"   ðŸ“Š Measurements pushed: {len(tuples):,}")
            print(f"   â±ï¸ Insert time: {insert_time:.2f}s")
            print(f"   â±ï¸ Total time: {total_time:.2f}s") 
            print(f"   ðŸš€ Throughput: {throughput:.0f} measurements/second")
            
            return True
            
        except Exception as e:
            print(f"âŒ Error in ultra-fast ClickHouse push: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def print_statistics(self):
        """Print comprehensive processing statistics"""
        print(f"\nðŸ“ˆ COMPREHENSIVE PROCESSING STATISTICS:")
        print("=" * 60)
        
        # Check if file was skipped
        if (hasattr(self, 'processing_stats') and 
            self.processing_stats.get('skipped_duplicate', False)):
            print(f"â­ï¸ FILE SKIPPED (DUPLICATE)")
            print(f"  Reason: File already processed")
            print(f"  File hash: {self.processing_stats.get('file_hash', 'N/A')}")
            print(f"  Processing time: 0.00s (no processing needed)")
            print("=" * 60)
            return
        
        # Parsing stats
        if self.processing_stats:
            print(f"C++ Parsing:")
            print(f"  Records parsed:        {self.processing_stats.get('total_records', 0):,}")
            print(f"  Measurements created:  {self.processing_stats.get('total_measurements', 0):,}")
            print(f"  C++ parsing time:      {self.processing_stats.get('cpp_parsing_time', 0):.2f}s")
            print(f"  Extraction time:       {self.processing_stats.get('measurement_extraction_time', 0):.2f}s")
            print(f"  Total parsing time:    {self.processing_stats.get('total_parsing_time', 0):.2f}s")
            
            # ClickHouse stats
            if 'total_clickhouse_time' in self.processing_stats:
                print(f"\nClickHouse Integration (clickhouse-connect):")
                print(f"  Transform time:        {self.processing_stats.get('clickhouse_transform_time', 0):.2f}s")
                print(f"  Schema setup time:     {self.processing_stats.get('clickhouse_setup_time', 0):.2f}s")
                print(f"  Data push time:        {self.processing_stats.get('clickhouse_push_time', 0):.2f}s")
                print(f"  Total ClickHouse time: {self.processing_stats.get('total_clickhouse_time', 0):.2f}s")
                print(f"  CH measurements:       {self.processing_stats.get('clickhouse_measurements', 0):,}")
                print(f"  CH landing records:    {self.processing_stats.get('clickhouse_landing_records', 0):,}")
            
            # Overall stats
            total_time = self.processing_stats.get('total_parsing_time', 0) + self.processing_stats.get('total_clickhouse_time', 0)
            if total_time > 0:
                throughput = self.processing_stats.get('total_measurements', 0) / total_time
                print(f"\nOverall Performance:")
                print(f"  Total processing time: {total_time:.2f}s")
                print(f"  Overall throughput:    {throughput:.2f} measurements/second")
                print(f"  Platform:              {platform.system()} ({platform.machine()})")
        
        print("=" * 60)


def main():
    """Main function with command-line interface"""
    parser = argparse.ArgumentParser(description="Extract measurements from STDF files with ClickHouse integration (Windows compatible)")
    parser.add_argument("--stdf-file", required=True, help="Path to STDF file")
    parser.add_argument("--push-clickhouse", action="store_true", help="Push results to ClickHouse")
    parser.add_argument("--batch-size", type=int, default=10000, help="ClickHouse batch size")
    parser.add_argument("--stdf-dir", help="Process all STDF files in directory")
    
    # ClickHouse connection args
    parser.add_argument("--ch-host", default="localhost", help="ClickHouse host")
    parser.add_argument("--ch-port", type=int, default=9000, help="ClickHouse native TCP port (default 9000)")
    parser.add_argument("--ch-database", default="default", help="ClickHouse database")
    parser.add_argument("--ch-user", default="default", help="ClickHouse user")
    parser.add_argument("--ch-password", default="", help="ClickHouse password")
    
    args = parser.parse_args()
    
    print("ðŸš€ Extract ALL Measurements + ClickHouse Integration")
    print("   Ultra-Fast Native TCP Edition (clickhouse-driver) - OPTIMIZED + FIXED!")
    print("=" * 60)
    
    # Process single file or directory
    stdf_files = []
    
    if args.stdf_dir:
        if not os.path.exists(args.stdf_dir):
            print(f"âŒ Directory not found: {args.stdf_dir}")
            return 1
        stdf_files = [os.path.join(args.stdf_dir, f) for f in os.listdir(args.stdf_dir) if f.endswith('.stdf')]
        if not stdf_files:
            print(f"âŒ No .stdf files found in {args.stdf_dir}")
            return 1
    else:
        if not os.path.exists(args.stdf_file):
            print(f"âŒ File not found: {args.stdf_file}")
            return 1
        stdf_files = [args.stdf_file]
    
    print(f"ðŸ“ Processing {len(stdf_files)} STDF file(s)")
    
    total_start = time.time()
    overall_measurements = 0
    successful_files = 0
    
    for stdf_file in stdf_files:
        try:
            print(f"\nðŸ“‚ Processing: {os.path.basename(stdf_file)}")
            
            # Create processor
            processor = STDFProcessor(
                enable_clickhouse=args.push_clickhouse,
                batch_size=args.batch_size
            )
            
            # Extract measurements with ClickHouse connection parameters
            measurements = processor.extract_measurements(
                stdf_file,
                ch_host=args.ch_host,
                ch_port=args.ch_port,
                ch_database=args.ch_database,
                ch_user=args.ch_user,
                ch_password=args.ch_password
            )
            
            # Check if file was skipped due to duplication
            if (hasattr(processor, 'processing_stats') and 
                processor.processing_stats.get('skipped_duplicate', False)):
                print(f"â­ï¸ File {os.path.basename(stdf_file)} skipped (already processed)")
                successful_files += 1  # Count as successful since it was handled properly
                continue
            
            overall_measurements += len(measurements)
            
            # Push to ClickHouse if requested
            if args.push_clickhouse:
                success = processor.push_to_clickhouse(
                    stdf_file,
                    host=args.ch_host,
                    port=args.ch_port,
                    database=args.ch_database,
                    user=args.ch_user,
                    password=args.ch_password
                )
                if success:
                    print(f"âœ… ClickHouse integration completed for {os.path.basename(stdf_file)}")
                else:
                    print(f"âŒ ClickHouse integration failed for {os.path.basename(stdf_file)}")
            
            # Print statistics
            processor.print_statistics()
            successful_files += 1
            
        except Exception as e:
            print(f"âŒ Error processing {stdf_file}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # Overall summary
    total_time = time.time() - total_start
    print(f"\nðŸŽ¯ OVERALL SUMMARY:")
    print("=" * 40)
    print(f"Files processed:       {successful_files}/{len(stdf_files)}")
    print(f"Total measurements:    {overall_measurements:,}")
    print(f"Total time:            {total_time:.2f}s")
    if total_time > 0:
        print(f"Overall throughput:    {overall_measurements/total_time:.2f} measurements/second")
    print(f"ClickHouse integration: {'âœ… Enabled' if args.push_clickhouse else 'âŒ Disabled'}")
    print(f"Platform:              {platform.system()} ({platform.machine()})")
    
    return 0 if successful_files > 0 else 1


if __name__ == "__main__":
    sys.exit(main())